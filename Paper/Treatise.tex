\input{preamble}

\begin{document}

\title{A JIT-less, Register-Mapped, Statically-Typed Virtual Machine for Interpreters}
\author{Matthew Sainsbury}
\date{\today}

% Cover Page
\begin{titlepage}

		\maketitle
	
\end{titlepage}

\pagenumbering{roman}

% Prefaces
\nakedchapter{Acknowledgements}
	To all my loyal fans

\nakedchapter{Abstract}
	\todo{align this with project proposal}
	Traditional JIT-less high-level register virtual machines simulate virtual registers in random access memory (RAM) and load virtual registers from RAM whenever they are accessed \citep{caseregistervm}. This treatise investigates an alternative approach where virtual registers are mapped to physical registers, and instructions are dispatched not only on the opcode, but also on the operands of the instruction. To emulate a virtual instruction, the interpreter jumps to the appropriate code segment in a table of implementation code based on the instruction word to be executed. This table grows in a polynomial fashion with increased virtual registers, and therefore the performance balance between table size and register pressure is investigated.

\nakedchapter{Declaration of Own Work}


\tableofcontents

\nakedchapter{Glossary}
\begin{description}
	\item[Bytecode] Program code executed by a virtual machine.
	\item[Virtual Machine] A class of programs which translate code written for one architecture to another.
\end{description}


% Main document body
\chapter{Introduction}
	\startrealnumbers
	This treatise investigates a new implementation technique for interpreted virtual machines. Two ideas are presented: register mapping between virtual machine registers and real registers, and a new dispatch method which operates on full instruction words. It is thought \todo{Weazel words?} that these two novelties might make it easier for a modern superscalar CPU to predict the behaviour of the virtual machine by reducing the amount of data dependance.
	
	This section aims to establish why such an endeavour is worthwhile by presenting a brief outline of the many existing designs for high level virtual machines.
	
	\section{Background}
		A virtual machine is a program that executes a machine code program by translating the instructions from one machine architecture to another. This allows a programmer to execute a program written for one architecture on a different architecture.
		
		A subclass of virtual machines---high-level virtual machines---are of interest in this project. A high level virtual machine runs programs for a machine which is imagined by its designer. This machine is typically more sophisticated and generic than real machines. As a consequence, a high-level virtual machine is an abstraction layer which hides the intricacies of any particular real machine. This virtual machine can be implemented for any physical architecture, resulting in the extremely useful property that code written for a virtual machine is very portable. The ``machine code'' that a high-level virtual machine executes is called its \emph{bytecode}. 
		
		The primary use for high-level virtual machines is in high-level interpreted languages, such as C$^\sharp$, Java and Python. High-level language interpreters are typically implemented with a compiler that compiles the input source code into an intermediary bytecode, which is then interpreted on a high level virtual machine. This technique is much faster than interpreting the source code line-by-line. Because the virtual machine is designed to support high-level features of interpreted languages, it is easier to write a compiler for the language targeting the virtual machine. This approach also helps to modularise the structure of the interpreter by seperating the language and context analysis structures from the host-specific implementation details \citep{structureinterpreters}.
		
		The interpretation of bytecode on any virtual machine is significantly slower than executing native code \citep{optimizingindirectbranch}. Just-in-time (JIT) compilation tries to bring ``the best of both worlds'' together. Modern JIT interpreters begin with the normal interpretation process, but while interpreting, profiles the executing bytecode to determine which parts of the bytecode would most benefit from native execution. Once it has identified these sections of bytecode, it compiles them into the host machine's instruction set and executes them natively instead of interpreting them \citep{historyjit}. Most popular virtual machines utilise JIT compilation, such as Java's JVM, and C$^\sharp$'s Common Language Runtime (CLR).
		
		Although JIT compilation is a good strategy to improve the performance of virtual machines, they have disadvantages in certain use cases. For instance, in multi-instance programs, where many identical threads having the same code, are executed simultaneously. A real-world example of such a program is a web server, which spawns several identical program threads to serve web clients asynchronously. Unfortunately, multi-instance applications have become much more common as CPUs increase in core count, and stagnate in clock frequency.
		
		Native multi-instance programs share a read-only code space between instances. This is not possible in JIT interpreters because a JIT compiler alters the program code as it runs. A JIT compiler cannot do this with a shared code space because an in-progress JIT compilation might interfere with a different thread executing the same part of the program. The alternative is to maintain a separate code space for each process, which is wasteful because it means that the JIT compiler will compile interpreted code separately for every running instance---essentially compiling the same code over and over. It is useful to explore opportunities for improvement in JIT-less interpreters so that situations where JIT compilation is inappropriate are not neglected. With this in mind, ``traditional'' high-level virtual machines will be considered.
		
		\subsubsection{Traditional Virtual Machines}
		There are two general types of virtual machine architectures: stack machines and register machines. 
			
		Unsurprisingly, a stack machine maintains a stack data structure where temporary values are stored, and the machine's instruction set consists mostly of operations on the stack. Figure \ref{fig:stackprogram} shows an example of code written for a stack machine. Notice that the operands of the instructions are implicit, and so instructions for stack machines tend to be shorter. This is the major difference between instructions for stack machines and register machines. 
		
		The bytecode of a stack machine is very simple. For each operation, a unique code (called an \emph{opcode}) is assigned. For instance, a word value~(``instruction word'') of \texttt{0x01} might represent push, while \texttt{0x02} might represent add, and so on. The introduction of literals, such as the $14$ and $8$ in the first two lines---as well as other types of additional data attached to instructions---complicates this picture, but the idea remains unchanged.
		
		A register machine contains a number of high-speed memory ``boxes'' into which temporary values are stored, and instructions generally specify which registers are involved in the operation~("operands"). Figure \ref{fig:registerprogram} shows the same program for a register machine. In the \texttt{add} instruction, the value of \texttt{regA} is added to \texttt{regB}, and the result is stored in \texttt{regA}. This is a convention in assembly programming which will be used throughout the text.
		
		A register machine's bytecode is different, because the operands of the instruction need to be part of the instruction encoding. Typically this is implemented using bitfields. For example, consider a twelve-bit instruction word. We can reserve the first 4 bits for the opcode. This gives us a maximum of 16 opcodes. The next four bits of the instruction word can encode the first operand as an enumeration, and the same for the last four bits which can encode the second operand. If an instruction only has one operand, the last four bits are undefined. This is illustrated in Figure \ref{fig:bitfields}
		
		\begin{figure}
			\hrule
			\begin{minipage}{\textwidth}
				\texttt{
					\\\\
					\begin{tabular}{ | l | r | r |r | }
						\hline
						Meaning & opcode & dest & src \\ 
						\hline
						add regA, regB  & 0001 & 0000 & 0001 \\
						\hline
						add regD, regC & 0001 & 0011 & 0010 \\
						\hline
						divide regB, regA & 0010 & 0001 & 0000 \\
						\hline
						print regA & 0011 & 0000 & 0000 \\
						\hline
					\end{tabular}
				}
				\caption{Register instructions using bitfields}
				\label{fig:bitfields}
			\end{minipage}
			\\\\\hrule
		\end{figure}
		
		\begin{figure}
			\hrule
			\begin{minipage}[b]{0.45\textwidth}
				\begin{lstlisting}
push 14
push 8
add
push 7
divide
print
				\end{lstlisting}
				\caption{Typical Stack machine program}
				\label{fig:stackprogram}
			\end{minipage}\hfill
			\begin{minipage}[b]{0.45\textwidth}
				\begin{lstlisting}
mov regA, 14
mov reg1, 8
add regA, regB
mov regB, 7
divide regA, regB
print regA
				\end{lstlisting}
				\caption{Typical Stack machine program}
				\label{fig:registerprogram}
			\end{minipage}
			\newline\hrule
		\end{figure}
		
	\section{Problem Description}
		Most physical machines are register machines. It may seem like a good idea to try to map virtual registers to real host registers. Unfortunately, unlike virtual registers that are implemented in host memory, real registers cannot be accessed by "reference" \citep{caseregistervm}. This difference complicates the implementation of a register-mapped virtual machine. Despite these difficulties, it is worthwhile to investigate designs of JIT-less register virtual machines that attempt to narrow the divide between the host and the guest architecture.
	
	\section{Purpose and Scope}
		The goal of this project is to investigate the feasibility of register virtual machines for interpreters running on a modern architecture. To this end, a high-level register virtual machine will be implemented.
	
		This virtual machine will have some unique design details that have not before been investigated or thought feasible. It will map some virtual machine registers to real registers. The virtual machine will perform dispatch not only on the opcodes of instructions, but on the entire instruction word including operands. 
	
		An instruction set architecture will be designed which will be tailored for such a dispatch method. Naturally the goal will be to keep the number of unique instructions down, so that this code table is as small as possible, while remaining non-trivial.
		
		\subsection{Scope}
		The virtual machine will target the Intel 64 host machine architecture, and will be written mostly in assembly, with some operating system-facing code written in C for ease of development. 
		
		\subsection{Limitations}
			The primary usefulness of virtual machines is that they can be ported to other architectures, and can be expected to perform similarly well on those other architectures. Unfortunately, however, there is only time to implement the virtual machine design on a single architecture, albeit a very popular one. To ensure the usefulness of this paper, a general comment about how the virtual machine is expected to perform on other architectures will be made, and a few suggestions of important considerations for implementation on architectures that are sufficiently  different from Intel 64 will be presented.
				
	\section{Overview of Treatise Structure}
		\todo{Will have to wait until the rest of the treatise exists for this!}
	
% Optional
%\chapter{Requirements Gathering}
%	\todo{Leaning towards removing this section}

% 10-12 pages
\chapter{Problem Exposition}
	\section{Literature Review}
		\cite{structureinterpreters} reflect on why spending research and engineering effort on optimising interpreters is a worthwhile endeavour. Creating a host machine independent abstraction layer between original source code and machine architecture greatly simplifies writing compilers that must generate code for many platforms. This is because for every additional platform that must be supported, a separate version of the compiler targeting that platform must be developed. In a design incorporating an interpreter, only the interpreter portion needs to be rewritten for different platforms, (or perhaps not even the interpreter if a high-level language is used to implement the interpreter) and the compiler remains the same. Interpreters tend to be simple compared to compilers, so this advantage is quite significant. Interpreters also offer opportunity for more powerful usability features like debuggers compared to machine code, and have slower compilers, impacting on the write-compile-test cycle of modern software development methods.
		

		
		Traditionally, the high level virtual machines implemented in interpreters are stack machines. Stack machines are used over register machines for a few reasons. Instructions for stack machines are smaller, because instruction operands are implicit. This means that instruction fetching is faster. It is easier to compile code for a stack machine than a register machine because the compiler does not need to implement a register allocator \citep{caseregistervm}. However, \cite{stackregistershowdown} found that an efficient register virtual machine can execute benchmarks 32\% faster than an analogous stack machine. 
		
		\cite{caseregistervm} mentions that the first successful virtual machine interpreter ran stack-based P-code for Pascal. This was the first in a line of stack-based bytecodes for popular languages like Smalltalk, Java and the Common Intermediate Language (CIL). This suggests that there might be historic factors involved in the prevalence of stack based virtual machines.
		
		\cite{fastjava} believes that interpreters are not optimised for modern architectures features like pipelining, branch prediction and caches. They also provide encouragement by saying, ``if interpreters can be made much faster, they will become suitable for a wide range of applications that currently need a JIT.''
		
		%mark
		
		The practice of mapping virtual machine resources to host machine registers has been investigated for stack machines by \cite{stackcaching}. Ertl measured the performance of stack machines which kept the top few elements of the stack in registers. He found that direct mapping of stack positions to machine registers was only useful for the case where just the top element of the stack was cached in a register. Further caching resulted in too many load-store instructions to shuffle register values around when positions on the stack change.
		
		Ertl also attempted a technique he calls ``dynamic stack caching'' where stack elements are cached in whatever register is convenient, and an internal state machine remembers the mapping between stack positions and registers. Many versions of code to emulate each virtual machine instructions exist; the version which is executed depends on the current state of the state machine. Ertl found that this technique halves the cost of operand fetching for each additional register involved in the caching, but adds an instruction dispatch cost because of the increased number of states.
		
		In the same paper, Ertl writes the first mention of the idea of achieving register mapping between a register virtual machine and a host register machine through a table of implementations for each combination of opcode and operands, but quickly dismisses it because it would ``cause code explosion, and will probably suffer a severe performance hit on machines with small first-level caches.'' He then went on to use the R4000 MIPS processor as an example, having 8 KB of level one (L1) instruction cache.
	
		However, this approach deserves further investigation, especially with the advent of modern processors with significantly larger cache sizes---for example, 4th generation Intel Core processors, which have 32 KB of L1 instruction cache \citep{haswellarch}, four times as much as the R4000.
	
		\todo{Talk about JIT - This is the approach taken by many popular interpreters, such as the Java Virtual Machine and Microsoft's Common Language Runtime.}
	\section{Difficulties}
		Interpreters spend most of their execution time on instruction dispatch \citep{modernarchvm}. The main contributor is usually an indirect branch to the implementation code for the currently executing virtual machine instruction \citep{optimizingindirectbranch}. Every interpreter has at least one indirect branch in dispatch code \citep{modernarchvm}. Because these indirect branches are in the instruction dispatch portion of the interpreter, they are executed for every virtual machine instruction. This closely couples interpreter speed to efficiency of branching.
		
		Modern architectures tend to have long pipelines and perform branch target prediction to load the target of an upcoming branch before the branch instruction is executed. Branch target misprediction is very expensive in these architectures, because the execution of a branch happens late in the pipeline but affects the start of the pipeline \citep{optimizingindirectbranch}. Traditional interpreter designs that were optimal on older architectures perform poorly on modern pipelined architectures because branch prediction accuracy is a big factor on performance on these architectures. These interpreter designs hide the logic governing branching patterns from the branch predictor. \cite{yeti} explains this by saying ``the control transfer from one body to the next is data dependent on the sequence of instructions making up the virtual program.'' This is an atypical scenario for modern processors, and they are not designed optimally for this task.
		
		To be more concrete, a popular method to predict indirect branch targets is a branch prediction buffer (BTB). BTB makes the assumption that the targets of indirect branches are unlikely to change \citep{yeti}, and maintains a table of the last target of each branch instruction. When the branch is encountered again, BTB predicts the target will remain the same. However, this is not the case for interpreter dispatch, because the indirect branch is based on the current virtual machine opcode, which could be any one of the instructions in the virtual machine instruction set. For each of these opcodes, the interpreter will branch to a particular section of code to emulate the virtual machine instruction. In other words, the BTB prediction will be accurate only if the opcode being dispatched is the same as the previous dispatch. Because it is not very likely that the virtual machine will see many of the same opcode over and over, the BTB fails to predict the branch target most of the time. Opcodes ``fight'' for the single target record in the BTB that corresponds with the indirect branch in the dispatch code.
		
		\cite{structureinterpreters} found that the branch misprediction penalty consumes up to half of the execution time on some interpreters. This high proportion is a result of the fact that most of an interpreter's time is spent in the instruction dispatch process.
		
		One way to get around this problem is by duplicating the indirect branch instruction, resulting in a separate BTB entry for each branch instruction \citep{fastjava}. This can be done by duplicating the dispatch code at the end of each virtual instruction implementation, instead of jumping back to a common dispatch code. In contrast, the traditional focus in branch optimisation was to make the code path as short as possible.
		
% 10-12 pages
\chapter{Solution Design}
	When designing the virtual machine architecture, it will be beneficial to remember this succinct heuristic by \cite{structureinterpreters}: ``Well-designed VMs are tailored for both easy compilation from the source language and fast interpretation on real machines.''\todo{too fluffy?}
	\todo{Biggest factor for performance is the design, so we spent a lot of time on it}

\chapter{Evaluation Methods and Results}

% Last chapter
\chapter{Conclusion}
	
	\section{Opportunities for Future Development}
	
	\section{Reflection}

% Bibliography
\bibliographysection

\end{document}